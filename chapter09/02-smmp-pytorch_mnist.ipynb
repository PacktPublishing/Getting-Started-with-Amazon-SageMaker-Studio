{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed using the `Python 3 (Data Science)` kernel on an `ml.t3.medium` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker-studio-book/chapter09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/smmp_pytorch_mnist.py\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import smdistributed.modelparallel.torch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchnet.dataset import SplitDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Make cudnn deterministic in order to get the same losses across runs.\n",
    "# The following two lines can be removed if they cause a performance impact.\n",
    "# For more details, see:\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html#cudnn\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fix the randomness\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)    \n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=4)\n",
    "    parser.add_argument('--num_workers', type=float, default=1)\n",
    "    # model directory /opt/ml/model default set by SageMaker\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GroupedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GroupedNet, self).__init__()\n",
    "        self.net1 = Net1()\n",
    "        self.net2 = Net2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.net2(x)\n",
    "        return x\n",
    "\n",
    "@smp.step\n",
    "def train_step(model, data, target):\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction='mean')\n",
    "    model.backward(loss)\n",
    "    \n",
    "    return output, loss\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move input tensors to the GPU ID used by the current process,\n",
    "        # based on the set_device call.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # loss_mb is a StepOutput object\n",
    "        _, loss_mb = train_step(model, data, target)\n",
    "\n",
    "        # Average the loss across microbatches.\n",
    "        loss = loss_mb.reduce_mean()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if smp.rank() == 0 and batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "            )\n",
    "\n",
    "\n",
    "# Define smp.step for model evaluation.\n",
    "@smp.step\n",
    "def test_step(model, data, target):\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target, reduction='sum').item() \n",
    "    pred = output.argmax(dim=1, keepdim=True) \n",
    "    correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    return loss, correct\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # SM Distributed: Moves input tensors to the GPU ID used by the current process\n",
    "            # based on the set_device call.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Since test_step returns scalars instead of tensors,\n",
    "            # test_step decorated with smp.step will return lists instead of StepOutput objects.\n",
    "            loss_batch, correct_batch = test_step(model, data, target)\n",
    "            test_loss += sum(loss_batch)\n",
    "            correct += sum(correct_batch)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if smp.mp_rank() == 0:\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100.0 * correct / len(test_loader.dataset),\n",
    "            )\n",
    "        )\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args, _ = parse_args()\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        raise ValueError('The script requires CUDA support, but CUDA not available')\n",
    "\n",
    "    smp.init()\n",
    "\n",
    "    # Set the device to the device on the current process (smp.local_rank).\n",
    "    torch.cuda.set_device(smp.local_rank())\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), \n",
    "         transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # Download only on a the first process per instance.\n",
    "    # When this is not present, the file is corrupted by multiple processes trying\n",
    "    # to download and extract at the same time\n",
    "    if smp.local_rank() == 0:\n",
    "        dataset1 = datasets.MNIST('../data', train=True, \n",
    "                                  download=True, transform=transform)\n",
    "        \n",
    "    # Wait for all processes to be ready\n",
    "    smp.barrier()\n",
    "    dataset1 = datasets.MNIST('../data', train=True, \n",
    "                              download=False, transform=transform)\n",
    "\n",
    "    # Download and create dataloaders for train and test dataset\n",
    "    dataset2 = datasets.MNIST('../data', train=False, \n",
    "                              transform=transform)\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size, 'num_workers': args.num_workers, \n",
    "              'pin_memory': True, 'shuffle': False, 'drop_last': True}\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
    "\n",
    "    # Get the model\n",
    "    model = GroupedNet()\n",
    "\n",
    "    # SageMaker Model Parallel handles the transfer of model parameters to the right device\n",
    "    # and the user doesn't need to call 'model.to(device)' explicitly.\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # Wrap the model with smp.DistributedModel and let smp handles the model parallelism\n",
    "    # Similiarly for optimizer\n",
    "    model = smp.DistributedModel(model)\n",
    "    optimizer = smp.DistributedOptimizer(optimizer)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "    for epoch in range(args.epochs):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Waiting the save checkpoint to be finished before run another allgather_object\n",
    "    smp.barrier()\n",
    "\n",
    "    # To save a model, always save on dp_rank 0 to avoid data racing\n",
    "    if smp.dp_rank() == 0:\n",
    "        model_dict = model.local_state_dict()\n",
    "        opt_dict = optimizer.local_state_dict()\n",
    "        model = {'model_state_dict': model_dict, 'optimizer_state_dict': opt_dict}\n",
    "        model_output_path = f'{args.model_dir}/pt_mnist_checkpoint.pt'\n",
    "        smp.save(model, model_output_path, partial=True)\n",
    "        # partial=True if you want to be able to load and further train a model that you save with smp.save()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "experiment_name = 'mnist-classification'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Training a classification model for mnist dataset.')\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'mnist-smmp-pt-{exp_datetime}'\n",
    "\n",
    "s3_output_location = f's3://{bucket}/{prefix}/{jobname}'\n",
    "code_dir = f's3://{bucket}/{prefix}/{jobname}'\n",
    "\n",
    "train_instance_type = 'ml.p3.8xlarge'\n",
    "\n",
    "distribution = {'smdistributed': {\n",
    "                    'modelparallel': {\n",
    "                        'enabled': True,\n",
    "                        'parameters': {\n",
    "                            'partitions': 2,\n",
    "                            'microbatches': 4,\n",
    "                            'pipeline': 'interleaved',\n",
    "                            'optimize': 'speed',\n",
    "                            'ddp': False\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': 2\n",
    "                }\n",
    "            }\n",
    "\n",
    "estimator = PyTorch(source_dir='code',\n",
    "                    entry_point='smmp_pytorch_mnist.py',\n",
    "                    output_path=s3_output_location,\n",
    "                    code_location=code_dir,\n",
    "                    instance_type=train_instance_type,\n",
    "                    instance_count=1,\n",
    "                    enable_sagemaker_metrics=True,\n",
    "                    sagemaker_session=sess,\n",
    "                    role=role,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py36',\n",
    "                    distribution=distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will use the estimator to launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                         trial_name=jobname)\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': exp_trial.trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "estimator.fit(job_name=jobname,\n",
    "              experiment_config=experiment_config,\n",
    "              wait=True)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
