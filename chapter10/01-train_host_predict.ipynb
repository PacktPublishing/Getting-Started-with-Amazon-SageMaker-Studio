{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed using the `Python 3 (Data Science)` kernel on an `ml.t3.medium` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "local_prefix='abalone'\n",
    "prefix = f'sagemaker-studio-book/chapter10/{local_prefix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import json, os, re, uuid\n",
    "from time import sleep, gmtime, strftime\n",
    "from threading import Thread\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.processing import ProcessingJob\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names taken from https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.names\n",
    "columns = ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', \n",
    "           'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings']\n",
    "df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', \n",
    "               names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "# Convert Rings to float so that model prediction (regression) and \n",
    "# the ground truth are both of float type for model monitor to work with\n",
    "df_processed['Rings']=df_processed['Rings'].astype(float)\n",
    "df_processed['Sex'] = df_processed['Sex'].replace(to_replace=['M', 'F', 'I'], \n",
    "                                                  value=[2., 1., 0.])\n",
    "# moving the target Rings to the first so that we can train with XGBoost.\n",
    "columns=['Rings', 'Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', \n",
    "         'ShuckedWeight', 'VisceraWeight', 'ShellWeight']\n",
    "df_processed = df_processed[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_build, df_test = train_test_split(df_processed, test_size=0.1, random_state=42, \n",
    "                                     shuffle=True, stratify=df_processed['Sex'])\n",
    "df_train, df_val = train_test_split(df_build, test_size=1/9., random_state=42, \n",
    "                                    shuffle=True, stratify=df_build['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_no_target = ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', \n",
    "                     'ShuckedWeight', 'VisceraWeight', 'ShellWeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(local_prefix, exist_ok=True)\n",
    "df_train.to_csv(f'./{local_prefix}/abalone_train.csv', index=False)\n",
    "df_val.to_csv(f'./{local_prefix}/abalone_val.csv', index=False)\n",
    "df_test.to_csv(f'./{local_prefix}/abalone_test.csv', index=False)\n",
    "\n",
    "desired_s3_uri = f's3://{bucket}/{prefix}/data'\n",
    "train_data_s3 = sagemaker.s3.S3Uploader.upload(local_path=f'./{local_prefix}/abalone_train.csv',\n",
    "                                               desired_s3_uri=desired_s3_uri,\n",
    "                                               sagemaker_session=sess)\n",
    "val_data_s3 = sagemaker.s3.S3Uploader.upload(local_path=f'./{local_prefix}/abalone_val.csv',\n",
    "                                             desired_s3_uri=desired_s3_uri,\n",
    "                                             sagemaker_session=sess)\n",
    "test_data_s3 = sagemaker.s3.S3Uploader.upload(local_path=f'./{local_prefix}/abalone_test.csv',\n",
    "                                              desired_s3_uri=desired_s3_uri,\n",
    "                                              sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a ML model to predict `Rings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_uris.retrieve(region=region, framework='xgboost', version='1.3-1')\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'abalone-xgb-{exp_datetime}'\n",
    "\n",
    "experiment_name = 'abalone-age-prediction'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Predicting age for abalone based on physical measurements.')\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n",
    "    \n",
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                         trial_name=jobname)\n",
    "\n",
    "experiment_config={'ExperimentName': experiment_name,\n",
    "                   'TrialName': exp_trial.trial_name,\n",
    "                   'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "train_s3_output = f's3://{bucket}/{prefix}/abalone_data/training'\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(image,\n",
    "                                    role,\n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    output_path=train_s3_output,\n",
    "                                    enable_sagemaker_metrics=True,\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(objective='reg:squarederror', num_round=20)\n",
    "\n",
    "train_input = sagemaker.inputs.TrainingInput(s3_data=train_data_s3, \n",
    "                                             content_type='csv')\n",
    "val_input = sagemaker.inputs.TrainingInput(s3_data=val_data_s3, \n",
    "                                           content_type='csv')\n",
    "data_channels={'train': train_input, 'validation': val_input}\n",
    "\n",
    "xgb.fit(inputs=data_channels, \n",
    "        job_name=jobname, \n",
    "        experiment_config=experiment_config, \n",
    "        wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model with data capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##S3 prefixes\n",
    "data_capture_prefix = f'{prefix}/datacapture'\n",
    "s3_capture_upload_path = f's3://{bucket}/{data_capture_prefix}'\n",
    "\n",
    "ground_truth_upload_path = f's3://{bucket}/{prefix}/ground-truth-data/{exp_datetime}'\n",
    "\n",
    "reports_prefix = f'{prefix}/reports'\n",
    "s3_report_path = f's3://{bucket}/{reports_prefix}'\n",
    "\n",
    "print(f'Capture path: {s3_capture_upload_path}')\n",
    "print(f'Ground truth path: {ground_truth_upload_path}')\n",
    "print(f'Report path: {s3_report_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(enable_capture=True, \n",
    "                                        sampling_percentage=100, \n",
    "                                        destination_s3_uri=s3_capture_upload_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'abalone-xgb-{exp_datetime}'\n",
    "print(f'EndpointName: {endpoint_name}')\n",
    "\n",
    "predictor = xgb.deploy(initial_instance_count=1,\n",
    "                       instance_type='ml.m5.large',\n",
    "                       endpoint_name=endpoint_name,\n",
    "                       serializer=CSVSerializer(),\n",
    "                       data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating prediction for validation set as model quality baseline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_np = Predictor(endpoint_name=endpoint_name, \n",
    "                         sagemaker_session=sess,\n",
    "                         serializer=CSVSerializer(),\n",
    "                         deserializer=CSVDeserializer())\n",
    "\n",
    "pred=predictor_np.predict(df_val[columns_no_target].values)\n",
    "pred_f = [float(i) for i in pred[0]]\n",
    "df_val['Prediction']=pred_f\n",
    "model_quality_baseline_suffix = f'{local_prefix}/abalone_val_model_quality_baseline.csv'\n",
    "df_val[['Rings', 'Prediction']].to_csv(model_quality_baseline_suffix, index=False)\n",
    "model_quality_baseline_s3 = sagemaker.s3.S3Uploader.upload(local_path=model_quality_baseline_suffix,\n",
    "                                                           desired_s3_uri=desired_s3_uri,\n",
    "                                                           sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a persistent load with randomness and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_randomness(series, probability = 0.1):\n",
    "    random_rate=(np.random.rand(series.shape[0])<probability).astype(float)\n",
    "    sigma_scale=0.5\n",
    "    \n",
    "    new_series = series * np.random.normal(loc=1, scale=sigma_scale*random_rate, \n",
    "                                           size=series.shape)\n",
    "    \n",
    "    if random_rate[0] != 1.:\n",
    "        # if random_rate for Sex (first cell in random_rate) is not 1,\n",
    "        # then assign a random value from [0,2].\n",
    "        new_series[0] = float(np.random.randint(0, 2))\n",
    "    else:\n",
    "        new_series[0] = series[0]\n",
    "\n",
    "    return new_series\n",
    "\n",
    "\n",
    "def drop_randomly(series, probability = 0.05):\n",
    "    random_rate=(np.random.rand(series.shape[0])<probability)\n",
    "    new_series = series.copy()\n",
    "    new_series[random_rate]=np.nan\n",
    "    \n",
    "    return new_series\n",
    "\n",
    "def convert_nparray_to_string(series):\n",
    "    new_series = ','.join([str(i) for i in series])\n",
    "    new_series = new_series.replace('nan', '')\n",
    "    \n",
    "    return new_series\n",
    "    \n",
    "def upload_ground_truth(records, ground_truth_upload_path, upload_time):\n",
    "    records_json = [json.dumps(r) for r in records]\n",
    "    data_to_upload = '\\n'.join(records_json)\n",
    "    target_s3_uri = f'{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl'\n",
    "    sagemaker.s3.S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_load_and_ground_truth():\n",
    "    gt_records=[]\n",
    "    for i, row in df_test.iterrows():\n",
    "        suffix = uuid.uuid1().hex\n",
    "        inference_id = f'{i}-{suffix}'\n",
    "        \n",
    "        gt = row['Rings']\n",
    "        data = row[columns_no_target].values\n",
    "        new_data = drop_randomly(add_randomness(data))\n",
    "        new_data = convert_nparray_to_string(new_data)\n",
    "        out = predictor.predict(data = new_data, inference_id = inference_id)\n",
    "\n",
    "        gt_data =  {'groundTruthData': {\n",
    "                            'data': str(gt), \n",
    "                            'encoding': 'CSV',\n",
    "                        },\n",
    "                    'eventMetadata': {\n",
    "                            'eventId': inference_id,\n",
    "                        },\n",
    "                    'eventVersion': '0',\n",
    "                    }\n",
    "        gt_records.append(gt_data)\n",
    "\n",
    "    upload_ground_truth(gt_records, ground_truth_upload_path, datetime.utcnow())\n",
    "    \n",
    "def generate_load_and_ground_truth_forever():\n",
    "    while True:\n",
    "        generate_load_and_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_load_and_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = Thread(target=generate_load_and_ground_truth_forever)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Test out the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get('Key') for capture_file in result.get('Contents')]\n",
    "print('Found Capture Files:')\n",
    "print('\\n '.join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[-2]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the next two cells to delete monitoring schedules and the endpoint to stop incurring cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## uncomment the lines below and run them to delete\n",
    "# client=sess.sagemaker_client\n",
    "# response=client.list_monitoring_schedules(EndpointName = endpoint_name)\n",
    "# for schedule in response['MonitoringScheduleSummaries']:\n",
    "#     schedule_name = schedule['MonitoringScheduleName']\n",
    "#     print(schedule_name)\n",
    "#     r = client.delete_monitoring_schedule(MonitoringScheduleName = schedule_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
