{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker-studio-book/chapter07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "x_train shape: (25000,)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 400\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "# csv_test_dir_prefix = 'imdb_data/test'\n",
    "# csv_test_filename = 'test.csv'\n",
    "# csv_test_dir = os.path.join(os.getcwd(), csv_test_dir_prefix)\n",
    "# os.makedirs(csv_test_dir, exist_ok=True)\n",
    "\n",
    "# np.savetxt(os.path.join(csv_test_dir, csv_test_filename), \n",
    "#            np.array(x_test, dtype=np.int32), fmt='%d', delimiter=\",\")\n",
    "\n",
    "# test_data_s3prefix = f'{prefix}/data/csv_test'\n",
    "# test_data_s3 = sagemaker.Session().upload_data(path=csv_test_dir, \n",
    "#                                                key_prefix=test_data_s3prefix)\n",
    "# print(test_data_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "training_job_name='imdb-tf-2021-09-21-17-37-20'\n",
    "\n",
    "estimator = TensorFlow.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "from time import gmtime, strftime\n",
    "\n",
    "experiment_name = 'imdb-sentiment-analysis'\n",
    "\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "jobname = f'imdb-tf-bt-{exp_datetime}'\n",
    "\n",
    "# Creating a new trial for the experiment\n",
    "exp_trial = Trial.load(\n",
    "    trial_name=training_job_name\n",
    ")\n",
    "\n",
    "experiment_config={\n",
    "    'ExperimentName': experiment_name,\n",
    "    'TrialName': exp_trial.trial_name,\n",
    "    'TrialComponentDisplayName': 'Inference-BatchTransform',\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(instance_count=1, \n",
    "                                    instance_type='ml.c5.xlarge',\n",
    "                                    max_payload = 6, \n",
    "                                    accept = 'text/csv', \n",
    "                                    assemble_with = 'Line')\n",
    "\n",
    "transformer.transform(test_data_s3, \n",
    "                      content_type='text/csv', \n",
    "                      split_type = 'Line', \n",
    "                      job_name = jobname,\n",
    "                      experiment_config = experiment_config)\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = transformer.output_path\n",
    "output_prefix = 'imdb_data/test_output'\n",
    "# !mkdir -p {output_prefix}\n",
    "# !aws s3 cp --recursive {output} {output_prefix}\n",
    "# !head {output_prefix}/csv-test.csv.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004, 1.0, 1.0, 0.4, 1.0, 1.0, 0.164, 0.101, 0.793, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.037, 1.0, 0.476, 1.0, 0.008, 0.476, 1.0, 0.0, 1.0, 0.4, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.228, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.824, 0.4, 0.4, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.4, 0.0, 0.991, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.023, 0.784, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.015, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 0.015, 0.009, 0.058, 0.4, 1.0, 1.0, 0.0, 0.4, 0.027, 0.149, 0.4, 0.713, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.988, 1.0, 0.0, 0.4, 0.0, 0.84, 0.022, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.008, 1.0, 0.0, 1.0, 0.0, 0.4, 0.4, 0.0, 0.003, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.999, 1.0, 0.867, 1.0, 0.008, 1.0, 0.0, 0.002, 0.4, 0.0, 1.0, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.032, 0.4, 0.0, 0.4, 0.4, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.001, 0.88, 1.0, 0.002, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.673, 0.4, 0.4, 0.0, 1.0, 0.0, 0.4, 0.283, 1.0, 0.4, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.003, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.065, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.049, 0.4, 0.4, 0.0, 0.4, 0.838, 1.0, 0.998, 0.014, 0.002, 1.0, 0.0, 1.0, 0.4, 0.992, 0.003, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.184, 0.4, 1.0, 1.0, 1.0, 0.026, 1.0, 0.009, 0.0, 0.0, 0.4, 0.4, 0.4, 0.4, 0.4, 1.0, 0.006, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.4, 0.237, 0.003, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.998, 0.093, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.942, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.004, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.288, 1.0, 1.0, 1.0, 0.892, 0.4, 0.0, 0.322, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.003, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.006, 0.0, 0.0, 1.0, 0.397, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.856, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.344, 1.0, 0.0, 0.0, 0.4, 1.0, 0.018, 0.007, 1.0, 0.0, 0.012, 0.232, 1.0, 0.0, 0.0, 1.0, 0.4, 0.092, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.026, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.997, 0.0, 0.157, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.002, 1.0, 0.089, 1.0, 0.0, 0.0, 0.4, 0.4, 0.0, 0.0, 0.081, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.003, 0.0, 0.0, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.043, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.644, 1.0, 0.781, 0.4, 0.4, 0.0, 1.0, 1.0, 0.001, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.157, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 0.0, 1.0, 0.4, 0.078, 0.0, 0.0, 0.0, 0.0, 1.0, 0.063, 0.0, 1.0, 0.0, 0.002, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.4, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.999, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.027, 1.0, 1.0, 0.653, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.001, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.418, 1.0, 1.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.002, 0.4, 0.196, 0.992, 0.4, 0.995, 0.033, 0.041, 0.013, 1.0, 1.0, 0.4, 0.4, 0.003, 0.0, 1.0, 0.001, 0.0, 1.0, 1.0, 0.0, 0.0, 0.986, 1.0, 1.0, 0.001, 1.0, 0.4, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.21, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.4, 1.0, 0.0, 0.0, 1.0, 0.2, 0.007, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.155, 1.0, 0.009, 1.0, 0.0, 0.303, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.906, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.005, 1.0, 0.0, 1.0, 0.054, 1.0, 1.0, 1.0, 0.964, 0.0, 0.014, 0.0, 1.0, 0.0, 0.958, 0.4, 0.001, 0.0, 1.0, 0.4, 0.064, 1.0, 1.0, 0.014, 0.4, 0.4, 1.0, 1.0, 0.0, 0.003, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.168, 1.0, 1.0, 0.324, 1.0, 1.0, 0.4, 0.4, 1.0, 0.0, 1.0, 0.011, 1.0, 1.0, 0.025, 1.0, 1.0, 1.0, 0.4, 0.001, 1.0, 0.0, 0.028, 0.4, 0.0, 0.4, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.02, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.901, 0.008, 0.0, 1.0, 1.0, 0.0, 0.003, 1.0, 0.377, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.995, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.021, 1.0, 0.004, 0.029, 1.0, 0.0, 1.0, 1.0, 0.215, 0.4, 0.0, 0.0, 0.001, 1.0, 0.0, 0.103, 0.0, 1.0, 0.01, 0.0, 0.827, 0.0, 0.0, 0.041, 1.0, 1.0, 1.0, 0.959, 1.0, 0.112, 0.0, 0.099, 0.0, 1.0, 0.0, 0.001, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.792, 0.4, 0.0, 1.0, 1.0, 1.0, 0.601, 0.4, 1.0, 0.001, 1.0, 0.0, 0.0, 0.4, 0.0, 0.381, 0.0, 0.238, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4, 0.0, 0.4, 0.999, 1.0, 0.003, 0.311, 0.002, 0.0, 1.0, 0.002, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.013, 0.0, 1.0, 1.0, 0.4, 1.0, 0.018, 0.964, 0.0, 0.4, 1.0, 0.0, 0.4, 0.063, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.135, 0.4, 0.0, 0.0, 0.0, 0.013, 1.0, 1.0, 0.999, 1.0, 0.0, 0.4, 1.0, 0.001, 1.0, 1.0, 0.4, 0.24, 1.0, 1.0, 0.441, 0.002, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.277, 0.4, 1.0, 0.4, 0.002, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4]\n"
     ]
    }
   ],
   "source": [
    "with open(f'{output_prefix}/csv-test.csv.out', 'r') as f:\n",
    "    json_output = json.load(f)\n",
    "    results = [float('%.3f'%(item)) for sublist in json_output['predictions'] \n",
    "                                    for item in sublist]\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(score):\n",
    "    return 'positive' if score > 0.5 else 'negative' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r'^[\\?\\s]+')\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i watched this movie purely for the setting it was filmed in an old hotel that a friend owns shares of the plot was predictable the acting was ? at best the scares were all gross outs not true scares br br i don't remember much of the plot and i think that's because there wasn't much of one to remember they didn't even use the hotel to it's fullest potential the beaches are fantastic and the hotel is situated on a ? at low tide you can walk almost 1 4 mile into the bay which is actually an eerie sight first thing in the morning or late at night when the wind is howling through the cracks br br the best way to see this movie is with the remote in your hand so you can fast forward through the action and i'm using that term ? scenes and pause at the beauty of the surroundings\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_index=199\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "first_decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') \n",
    "                                 for i in x_test[data_index]])\n",
    "regex.sub('', first_decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled sentiment for this review is negative\n",
      "Predicted sentiment is negative\n"
     ]
    }
   ],
   "source": [
    "print(f'Labeled sentiment for this review is {get_sentiment(y_test[data_index])}')\n",
    "print(f'Predicted sentiment is {get_sentiment(results[data_index])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
