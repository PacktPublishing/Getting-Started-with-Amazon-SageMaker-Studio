{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed using the `Python 3 (Data Science)` kernel on an `ml.t3.xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "from IPython.display import HTML \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker-studio-book/chapter07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir load_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile load_testing/locustfile.py\n",
    "from locust import task, between, events, User\n",
    "import sagemaker\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "endpoint_name=os.environ['ENDPOINT_NAME']\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name, \n",
    "                                          serializer=JSONSerializer(),\n",
    "                                          deserializer=JSONDeserializer())\n",
    "print(predictor.endpoint_name)\n",
    "\n",
    "csv_test_dir_prefix = 'imdb_data/test'\n",
    "csv_test_filename = 'test.csv'\n",
    "\n",
    "# loads a sample and make one inference call\n",
    "x_test = np.loadtxt(f'{csv_test_dir_prefix}/{csv_test_filename}', \n",
    "                    delimiter=',', dtype='int', max_rows=1)\n",
    "out = predictor.predict(x_test)\n",
    "print(out)\n",
    "\n",
    "class SMLoadTestUser(User):\n",
    "    wait_time = between(0, 1)\n",
    "    \n",
    "    @task\n",
    "    def test_endpoint(self):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            predictor.predict(x_test)\n",
    "            total_time = int((time.time() - start_time) * 1000)\n",
    "            events.request_success.fire(\n",
    "                request_type=\"sagemaker\",\n",
    "                name=\"predict\",\n",
    "                response_time=total_time,\n",
    "                response_length=0)\n",
    "\n",
    "        except:\n",
    "            total_time = int((time.time() - start_time) * 1000)\n",
    "            events.request_failure.fire(\n",
    "                request_type=\"sagemaker\",\n",
    "                name=\"predict\",\n",
    "                response_time=total_time,\n",
    "                response_length=0,\n",
    "                exception=sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: load testing original instance configuration (one ml.c5.xlarge)\n",
    "We assume you have run the [02-tensorflow_sentiment_analysis_inference.ipynb](./02-tensorflow_sentiment_analysis_inference.ipynb) and created an endpoint using ml.c5.xlarge instance. You should fill in the endpoint name and reuse the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = sess.boto_session.client('sagemaker')\n",
    "autoscaling_client = sess.boto_session.client('application-autoscaling')\n",
    "\n",
    "endpoint_name = '<endpoint-with-ml.c5-xlarge-instance>'\n",
    "resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic' \n",
    "response = autoscaling_client.register_scalable_target(\n",
    "   ServiceNamespace='sagemaker',\n",
    "   ResourceId=resource_id,\n",
    "   ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "   MinCapacity=1,\n",
    "   MaxCapacity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh --bg\n",
    "export ENDPOINT_NAME='<endpoint-with-ml.c5-xlarge-instance>'\n",
    "bind_port=5557\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} & \n",
    "\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n",
    "\n",
    "locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \\\n",
    "       --print-stats --only-summary --loglevel ERROR \\\n",
    "       --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_url=f\"https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'{endpoint_name}~'VariantName~'AllTraffic)~(~'.~'ModelLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocations~'.~'.~'.~'.)~(~'.~'OverheadLatenc-y~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocation5XXErrors~'.~'.~'.~'.)~(~'.~'Invocation4XXErrors~'.~' .~'.~'.))~view~'timeSeries~stacked~false~region~'us-east-1~stat~'Sum~period~60~start~'-PT3H~end~'P0D);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{endpoint_name}\"\n",
    "HTML(f'<a href=\"{cw_url}\">Click here to open the CloudWatch dashboard</a>') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2: load test scaled up configuration (one ml.c5.2xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# insert the training job name from chapter05/02-tensorflow_sentiment_analysis.ipynb\n",
    "training_job_name='<your-training-job-name>'\n",
    "\n",
    "estimator = TensorFlow.attach(training_job_name)\n",
    "predictor_c5_2xl = estimator.deploy(initial_instance_count=1, \n",
    "                                    instance_type='ml.c5.2xlarge',\n",
    "                                    wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_c5_2xl.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh --bg\n",
    "export ENDPOINT_NAME='<endpoint-with-ml.c5-2xlarge-instance>'\n",
    "bind_port=5558\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} & \n",
    "\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n",
    "\n",
    "locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \\\n",
    "       --print-stats --only-summary --loglevel ERROR \\\n",
    "       --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_url2=f\"https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'{predictor_c5_2xl.endpoint_name}~'VariantName~'AllTraffic)~(~'.~'ModelLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocations~'.~'.~'.~'.)~(~'.~'OverheadLatenc-y~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocation5XXErrors~'.~'.~'.~'.)~(~'.~'Invocation4XXErrors~'.~' .~'.~'.))~view~'timeSeries~stacked~false~region~'us-east-1~stat~'Sum~period~60~start~'-PT3H~end~'P0D);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor_c5_2xl.endpoint_name}\"\n",
    "HTML(f'<a href=\"{cw_url2}\">Click here to open the CloudWatch dashboard</a>') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 3: load test GPU instance dedicated to ML inference (one ml.g4dn.xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_g4dn_xl = estimator.deploy(initial_instance_count=1, \n",
    "                                     instance_type='ml.g4dn.xlarge',\n",
    "                                     wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_g4dn_xl.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh --bg\n",
    "export ENDPOINT_NAME='<endpoint-with-ml.g4dn-xlarge-instance>'\n",
    "bind_port=5559\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} & \n",
    "\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n",
    "\n",
    "locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \\\n",
    "       --print-stats --only-summary --loglevel ERROR \\\n",
    "       --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_url3=f\"https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'{predictor_g4dn_xl.endpoint_name}~'VariantName~'AllTraffic)~(~'.~'ModelLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocations~'.~'.~'.~'.)~(~'.~'OverheadLatenc-y~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocation5XXErrors~'.~'.~'.~'.)~(~'.~'Invocation4XXErrors~'.~' .~'.~'.))~view~'timeSeries~stacked~false~region~'us-east-1~stat~'Sum~period~60~start~'-PT3H~end~'P0D);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor_g4dn_xl.endpoint_name}\"\n",
    "HTML(f'<a href=\"{cw_url3}\">Click here to open the CloudWatch dashboard</a>') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 4: load test autoscaling (1-4 ml.c5.xlarge instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic' # This is the format in which application autoscaling references the endpoint\n",
    "\n",
    "response = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = autoscaling_client.describe_scaling_policies(ServiceNamespace='sagemaker')\n",
    "# print(response)\n",
    "for i in response['ScalingPolicies']:\n",
    "    print('')\n",
    "    print(i['PolicyName'])\n",
    "    print('')\n",
    "    if('TargetTrackingScalingPolicyConfiguration' in i):\n",
    "        print(i['TargetTrackingScalingPolicyConfiguration']) \n",
    "    else:\n",
    "        print(i['StepScalingPolicyConfiguration'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh --bg\n",
    "export ENDPOINT_NAME='<endpoint-with-ml.c5-xlarge-instance>'\n",
    "bind_port=5560\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} & \n",
    "\n",
    "locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &\n",
    "\n",
    "locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \\\n",
    "       --print-stats --only-summary --loglevel ERROR \\\n",
    "       --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(f'<a href=\"{cw_url}\">Click here to open the CloudWatch dashboard</a>') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the next cell to delete endpoints to stop incurring cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor_c5_2xl.delete_endpoint()\n",
    "# predictor_g4dn_xl.delete_endpoint()\n",
    "# !aws sagemaker delete-endpoint --endpoint-name {endpoint_name}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
