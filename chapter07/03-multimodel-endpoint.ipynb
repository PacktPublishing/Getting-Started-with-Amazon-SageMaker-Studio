{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is developed using the `Python 3 (Data Science)` kernel on an `ml.t3.medium` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'sagemaker-studio-book/chapter07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file='s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt'\n",
    "local_prefix='churn_data'\n",
    "os.makedirs(local_prefix, exist_ok=True)\n",
    "sagemaker.s3.S3Downloader.download(source_file, local_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(f'./{local_prefix}/churn.txt')\n",
    "df['CustomerID']=df.index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Int'l Plan\", \"VMail Plan\"]] = df[[\"Int'l Plan\", \"VMail Plan\"]].replace(to_replace=['yes', 'no'], value=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn?'] = df['Churn?'].replace(to_replace=['True.', 'False.'], value=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Churn?', 'State', 'Account Length', \"Int'l Plan\",\n",
    "           'VMail Plan', 'VMail Message', 'Day Mins', 'Day Calls', 'Day Charge',\n",
    "           'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls',\n",
    "           'Night Charge', 'Intl Mins', 'Intl Calls', 'Intl Charge',\n",
    "           'CustServ Calls']\n",
    "df.index = df['CustomerID']\n",
    "df_processed = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df_processed, test_size=0.1, random_state=42, \n",
    "                                     shuffle=True, stratify=df_processed['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_no_target=['Account Length', \"Int'l Plan\", 'VMail Plan', 'VMail Message', 'Day Mins',\n",
    "                   'Day Calls', 'Day Charge', 'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls',\n",
    "                   'Night Charge', 'Intl Mins', 'Intl Calls', 'Intl Charge', 'CustServ Calls']\n",
    "\n",
    "df_test.to_csv(f'{local_prefix}/churn_test.csv')\n",
    "df_test[columns_no_target].to_csv(f'{local_prefix}/churn_test_no_target.csv', \n",
    "                                  index=False)\n",
    "\n",
    "sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_test.csv', \n",
    "                               f's3://{bucket}/{prefix}/{local_prefix}')\n",
    "sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_test_no_target.csv', \n",
    "                               f's3://{bucket}/{prefix}/{local_prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "image = image_uris.retrieve(region=region, framework='xgboost', version='1.3-1')\n",
    "train_instance_type = 'ml.m5.xlarge'\n",
    "train_instance_count = 1\n",
    "s3_output = f's3://{bucket}/{prefix}/{local_prefix}/training'\n",
    "\n",
    "experiment_name = 'churn-prediction'\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name, \n",
    "        description='Training churn prediction models based on telco churn dataset.')\n",
    "except ClientError as e:\n",
    "    print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n",
    "    \n",
    "\n",
    "def launch_training_job(state, train_data_s3, val_data_s3):\n",
    "    exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "    jobname = f'churn-xgb-{state}-{exp_datetime}'\n",
    "\n",
    "    # Creating a new trial for the experiment\n",
    "    exp_trial = Trial.create(experiment_name=experiment_name, \n",
    "                             trial_name=jobname)\n",
    "\n",
    "    experiment_config={\n",
    "        'ExperimentName': experiment_name,\n",
    "        'TrialName': exp_trial.trial_name,\n",
    "        'TrialComponentDisplayName': 'Training'}\n",
    "\n",
    "    xgb = sagemaker.estimator.Estimator(image,\n",
    "                                        role,\n",
    "                                        instance_count=train_instance_count,\n",
    "                                        instance_type=train_instance_type,\n",
    "                                        output_path=s3_output,\n",
    "                                        enable_sagemaker_metrics=True,\n",
    "                                        sagemaker_session=sess)\n",
    "    xgb.set_hyperparameters(objective='binary:logistic', num_round=20)\n",
    "    \n",
    "    train_input = sagemaker.inputs.TrainingInput(s3_data=train_data_s3, \n",
    "                                                 content_type='csv')\n",
    "    val_input = sagemaker.inputs.TrainingInput(s3_data=val_data_s3, \n",
    "                                               content_type='csv')\n",
    "    data_channels={'train': train_input, 'validation': val_input}\n",
    "    \n",
    "    xgb.fit(inputs=data_channels, job_name=jobname, \n",
    "            experiment_config=experiment_config, wait=False)\n",
    "\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_estimator = {}\n",
    "for state in df_processed.State.unique()[:5]:\n",
    "    print(state)\n",
    "    output_dir = f's3://{bucket}/{prefix}/{local_prefix}/by_state'\n",
    "    df_state = df_train[df_train['State']==state].drop(labels='State', axis=1)\n",
    "    df_state_train, df_state_val = train_test_split(df_state, test_size=0.1, random_state=42, \n",
    "                                                    shuffle=True, stratify=df_state['Churn?'])\n",
    "    \n",
    "    df_state_train.to_csv(f'{local_prefix}/churn_{state}_train.csv', index=False)\n",
    "    df_state_val.to_csv(f'{local_prefix}/churn_{state}_val.csv', index=False)\n",
    "    sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_train.csv', output_dir)\n",
    "    sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_val.csv', output_dir)\n",
    "    \n",
    "    dict_estimator[state] = launch_training_job(state, out_train_csv_s3, out_val_csv_s3)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_training_job_to_complete(estimator):\n",
    "    job = estimator.latest_training_job.job_name\n",
    "    print(f\"Waiting for job: {job}\")\n",
    "    status = estimator.latest_training_job.describe()[\"TrainingJobStatus\"]\n",
    "    while status == \"InProgress\":\n",
    "        time.sleep(45)\n",
    "        status = estimator.latest_training_job.describe()[\"TrainingJobStatus\"]\n",
    "        if status == \"InProgress\":\n",
    "            print(f\"{job} job status: {status}\")\n",
    "    print(f\"DONE. Status for {job} is {status}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in list(dict_estimator.values()):\n",
    "    wait_for_training_job_to_complete(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "model_PA = dict_estimator['PA'].create_model(role=role, image_uri=image)\n",
    "model_data_prefix = f's3://{bucket}/{prefix}/{local_prefix}/multi_model_artifacts/'\n",
    "exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "model_name = f'churn-xgb-mme-{exp_datetime}'\n",
    "endpoint_name = model_name\n",
    "hosting_instance_type = 'ml.c5.xlarge'\n",
    "hosting_instance_count = 1\n",
    "\n",
    "mme = MultiDataModel(name=model_name,\n",
    "                     model_data_prefix=model_data_prefix,\n",
    "                     model=model_PA,  # passing our model - passes container image needed for the endpoint\n",
    "                     sagemaker_session=sess)\n",
    "predictor = mme.deploy(initial_instance_count=hosting_instance_count, \n",
    "                       instance_type=hosting_instance_type, \n",
    "                       endpoint_name=endpoint_name,\n",
    "                       serializer = CSVSerializer(),\n",
    "                       deserializer = JSONDeserializer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state, est in dict_estimator.items():\n",
    "    artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "    model_name = f'{state}.tar.gz'\n",
    "    print(model_name)\n",
    "    # This is copying over the model artifact to the S3 location for the MME.\n",
    "    mme.add_model(model_data_source=artifact_path, model_data_path=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_test_data(state):\n",
    "    sample = df_test[df_test['State']==state].sample(1)\n",
    "    sample[[\"Int'l Plan\", 'VMail Plan']]=sample[[\"Int'l Plan\", 'VMail Plan']].astype(int)\n",
    "    target = sample['Churn?'].values[0].tolist()\n",
    "    sample = sample.values[0][2:].tolist()    \n",
    "\n",
    "    return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "state='PA'\n",
    "test_data=sample_test_data(state)\n",
    "print(test_data[0])\n",
    "prediction = predictor.predict(data=test_data[0], \n",
    "                               target_model=f'{state}.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f'{prediction} vs ground truth {test_data[1]}')\n",
    "print('It took {:,d} ms\\n'.format(int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "state='SC'\n",
    "test_data=sample_test_data(state)\n",
    "print(test_data[0])\n",
    "prediction = predictor.predict(data=test_data[0], \n",
    "                               target_model=f'{state}.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f'{prediction} vs ground truth {test_data[1]}')\n",
    "print('It took {:,d} ms\\n'.format(int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "state='VA'\n",
    "test_data=sample_test_data(state)\n",
    "print(test_data[0])\n",
    "prediction = predictor.predict(data=test_data[0], \n",
    "                               target_model=f'{state}.tar.gz')\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f'{prediction} vs ground truth {test_data[1]}')\n",
    "print('It took {:,d} ms\\n'.format(int(duration * 1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the next cell to delete endpoints to stop incurring cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
